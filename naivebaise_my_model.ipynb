{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10db463b-d8cf-429d-a892-c69bee6b5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "00dc95c8-ac09-40d8-94c4-dc40bc8eae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data\n",
    "texts = [\n",
    "    \"I love this movie\",\n",
    "    \"This film is amazing\",\n",
    "    \"I hate this movie\",\n",
    "    \"This movie is boring\",\n",
    "]\n",
    "\n",
    "labels = ['positive', 'positive', 'negative', 'negative']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "00bca463-2254-4f86-8e35-08882c17d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "## text to vector then training the naive bayes algorithm to analyise for setimental analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cbfe285b-4a51-4c8b-9b19-b96db329287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_words = {}\n",
    "vis = set()\n",
    "i = 0\n",
    "for text in texts:\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        \n",
    "        word = word.lower()\n",
    "        if word not in vis:\n",
    "            vis.add(word)\n",
    "            uniq_words[word] = i\n",
    "            i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "60d4958e-fd23-4509-872d-5974355d1013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0,\n",
       " 'love': 1,\n",
       " 'this': 2,\n",
       " 'movie': 3,\n",
       " 'film': 4,\n",
       " 'is': 5,\n",
       " 'amazing': 6,\n",
       " 'hate': 7,\n",
       " 'boring': 8}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#these are will be the features\n",
    "uniq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2d78fadb-1966-496e-8f3c-0cb8a9f69a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v = []\n",
    "\n",
    "i = 0\n",
    "for text in texts:\n",
    "    words = word_tokenize(text)\n",
    "    v = [0]*len(uniq_words)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        v[uniq_words[word]] = 1\n",
    "    data_v.append(v)\n",
    "    i += 1\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "80901223-11fd-4a4a-9ee8-862cb08d2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e74f8d85-3f9f-41de-95b9-0e3a4789dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in labels:\n",
    "    if i == \"positive\":\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1be52e22-6f62-4f6f-bbee-76db8562bf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7df4a521-d28e-4056-b125-1ec7270ce0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ce7aa9fa-5907-4fdb-a8eb-b23f5837522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now i vectorise the data i applied the algo naivebayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "18cd8a26-dac6-4fed-a4c6-389fe49fccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "p_pos = sum(y)/len(y)\n",
    "p_neg = 1 - p_pos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c4826ad4-f0e7-4d4a-b8e7-3023adebda50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pos,p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1feafb45-d378-45cc-8387-29a7735f8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = uniq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "07731bce-9b69-420f-8d23-a8ef37f24c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0,\n",
       " 'love': 1,\n",
       " 'this': 2,\n",
       " 'movie': 3,\n",
       " 'film': 4,\n",
       " 'is': 5,\n",
       " 'amazing': 6,\n",
       " 'hate': 7,\n",
       " 'boring': 8}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fb056415-bb63-45a8-908b-efe267c3d842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = {}\n",
    "for i in range(len(features)):\n",
    "    y_in_y = 0\n",
    "    n_in_y = 0\n",
    "    for j in range(len(X)):\n",
    "        if X[j][i] == 1 and y[j] == 1:\n",
    "            y_in_y += 1\n",
    "        elif X[j][i] == 1 and y[j] ==0:\n",
    "            n_in_y += 1\n",
    "    model[i] = (y_in_y/sum(y),n_in_y/(len(y) - sum(y)))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fffd2fd1-88bd-44ec-a1da-61dac9db197d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0.5, 0.5),\n",
       " 1: (0.5, 0.0),\n",
       " 2: (1.0, 1.0),\n",
       " 3: (0.5, 1.0),\n",
       " 4: (0.5, 0.0),\n",
       " 5: (0.5, 0.5),\n",
       " 6: (0.5, 0.0),\n",
       " 7: (0.0, 0.5),\n",
       " 8: (0.0, 0.5)}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c3bf80fd-6043-4482-ae4c-2e1e6b4a63e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text,features,model,p_pos,p_neg):\n",
    "    words = word_tokenize(text)\n",
    "    w = set()\n",
    "    for word in words:\n",
    "        w.add(word.lower())\n",
    "    #calculating p_pos\n",
    "    p_pos = p_pos\n",
    "    p_neg = p_neg\n",
    "    for feature in features:\n",
    "        if feature in w:\n",
    "            p = model[features[feature]][0]\n",
    "            n = model[features[feature]][1]\n",
    "            if  p == 0:\n",
    "                p_pos *= 0.0000000001\n",
    "            else:\n",
    "                p_pos *= p\n",
    "                \n",
    "            if  n == 0:\n",
    "                p_neg *= 0.0000000001\n",
    "            else:\n",
    "                p_neg *= n\n",
    "            \n",
    "        else:\n",
    "            p = 1 - model[features[feature]][0]\n",
    "            n = 1 - model[features[feature]][1]\n",
    "            \n",
    "            if  p == 0:\n",
    "                p_pos *= 0.0000000001\n",
    "            else:\n",
    "                p_pos *= p\n",
    "                \n",
    "            if  n == 0:\n",
    "                p_neg *= 0.0000000001\n",
    "            else:\n",
    "                p_neg *= n\n",
    "\n",
    "    return \"Negative\" if p_neg > p_pos else \"Pos\"\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ce65a0ad-4b7a-4170-bc7a-8249a0ba44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model:\n",
    "    def __init__(self,texts,labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "        #converting word into vectors\n",
    "        uniq_words = {}\n",
    "        vis = set()\n",
    "        i = 0\n",
    "        for text in texts:\n",
    "            words = word_tokenize(text)\n",
    "            for word in words:\n",
    "                \n",
    "                word = word.lower()\n",
    "                if word not in vis:\n",
    "                    vis.add(word)\n",
    "                    uniq_words[word] = i\n",
    "                    i += 1\n",
    "                    \n",
    "        data_v = []\n",
    "        i = 0\n",
    "        for text in texts:\n",
    "            words = word_tokenize(text)\n",
    "            v = [0]*len(uniq_words)\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                v[uniq_words[word]] = 1\n",
    "            data_v.append(v)\n",
    "            i += 1\n",
    "\n",
    "        \n",
    "        y = []\n",
    "        for i in labels:\n",
    "            if i == \"positive\":\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "\n",
    "        self.p_pos = sum(y)/len(y)\n",
    "        self.p_neg = 1 - p_pos\n",
    "\n",
    "        features = uniq_words\n",
    "\n",
    "        model = {}\n",
    "        for i in range(len(features)):\n",
    "            y_in_y = 0\n",
    "            n_in_y = 0\n",
    "            for j in range(len(X)):\n",
    "                if X[j][i] == 1 and y[j] == 1:\n",
    "                    y_in_y += 1\n",
    "                elif X[j][i] == 1 and y[j] ==0:\n",
    "                    n_in_y += 1\n",
    "            model[i] = (y_in_y/sum(y),n_in_y/(len(y) - sum(y)))\n",
    "\n",
    "        self.model = model\n",
    "        self.features = uniq_words\n",
    "\n",
    "    def predict(self,text):\n",
    "        features = self.features\n",
    "        model = self.model\n",
    "        p_pos = self.p_pos\n",
    "        p_neg = self.p_neg\n",
    "        \n",
    "        words = word_tokenize(text)\n",
    "        w = set()\n",
    "        for word in words:\n",
    "            w.add(word.lower())\n",
    "        #calculating p_pos\n",
    "        p_pos = p_pos\n",
    "        p_neg = p_neg\n",
    "        for feature in features:\n",
    "            if feature in w:\n",
    "                p = model[features[feature]][0]\n",
    "                n = model[features[feature]][1]\n",
    "                if  p == 0:\n",
    "                    p_pos *= 0.0000000001\n",
    "                else:\n",
    "                    p_pos *= p\n",
    "                    \n",
    "                if  n == 0:\n",
    "                    p_neg *= 0.0000000001\n",
    "                else:\n",
    "                    p_neg *= n\n",
    "                \n",
    "            else:\n",
    "                p = 1 - model[features[feature]][0]\n",
    "                n = 1 - model[features[feature]][1]\n",
    "                \n",
    "                if  p == 0:\n",
    "                    p_pos *= 0.0000000001\n",
    "                else:\n",
    "                    p_pos *= p\n",
    "                    \n",
    "                if  n == 0:\n",
    "                    p_neg *= 0.0000000001\n",
    "                else:\n",
    "                    p_neg *= n\n",
    "\n",
    "        return \"Negative\" if p_neg > p_pos else \"Pos\"\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4f8f47b9-4446-46a3-b609-a5f15d235a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model(texts,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "657f7102-47de-402b-944e-5cf47cc4774d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"i hate this movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c10945-e450-4a8b-8196-9544cdb0cb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
